{"cells":[{"cell_type":"markdown","source":["Adding Backwards Compatibility"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"262134ee-b93d-459a-8907-068121db68be"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fcb3218-40df-4842-a3ce-1e1cd214058f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# %run \"../Includes/Function/GetNotebook\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a685bc52-e9c4-470a-bb32-7bdcd552d30c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\",sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b95d2be0-01e3-41aa-baea-11c40ce95305"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom datetime import datetime\nfrom delta.tables import *\nimport os\nimport json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7720df6f-eb03-4bb0-9881-7d64a1daa75b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Get Parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"880861b5-4764-444a-b9c2-dafa2ce4f693"}}},{"cell_type":"code","source":["# data vault load\ndbutils.widgets.text(\"Source_System\", \"\")\ndbutils.widgets.text(\"DataVault_Entity_Name\", \"\")\n\n# deletion capture\ndbutils.widgets.text(\"Deletion_Capture_Enabled\",\"\")\ndeletion_captured_enabled = dbutils.widgets.get(\"Deletion_Capture_Enabled\").lower()\n\n# refresh interval\ndbutils.widgets.text(\"Is_Full_Refresh\",\"\")\nis_full_refresh = dbutils.widgets.get(\"Is_Full_Refresh\").lower()\n\n# schema\ndbutils.widgets.text(\"Schema\",\"\")\nschema = dbutils.widgets.get(\"Schema\")\n\ndbutils.widgets.text(\"Is_Overwrite_Schema\",\"False\")\nis_overwrite_schema = dbutils.widgets.get(\"Is_Overwrite_Schema\")\n\ndbutils.widgets.text(\"Is_Merge_Schema\",\"True\")\nis_merge_schema = dbutils.widgets.get(\"Is_Merge_Schema\")\n\n# root path\ndbutils.widgets.text(\"Root_Path\", \"s3\")\nrootPath = dbutils.widgets.get(\"Root_Path\")\n\n# etl logging\ndbutils.widgets.text(\"ETL_ID\",\"\")\ndbutils.widgets.text(\"Data_Factory_Name\",\"\")\ndbutils.widgets.text(\"Pipeline_Name\",\"\")\ndbutils.widgets.text(\"Pipeline_Run_Id\",\"\")\n\netlID = dbutils.widgets.get(\"ETL_ID\")\ndataFactoryName = dbutils.widgets.get(\"Data_Factory_Name\")\npipelineName = dbutils.widgets.get(\"Pipeline_Name\")\npipelineRunId = dbutils.widgets.get(\"Pipeline_Run_Id\")\n\n#query\ndbutils.widgets.text(\"query_folder\",\"/dbfs/mnt/....\")\nquery_folder = dbutils.widgets.get(\"query_folder\")\n\n# #logging info\n# workSpaceName = GetNotebook().workspace()\n# notebookName = GetNotebook().notebook()\n# outputFileLoc = \"/databricks-results/\"\n# outputFile = os.path.join(outputFileLoc, workSpaceName, notebookName)\n# stepName = \"Load Data Vault\"\n\n# save data\ndbutils.widgets.text(\"Write_Mode\", \"append\")\nwrite_mode = dbutils.widgets.get(\"Write_Mode\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0dd2152-f79f-4b32-9b8e-09a0ad9ecb36"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Generate Hashkey Function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14a984ff-6e3a-41bc-813d-882ab9c4e6fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create Data Vault Entity Function "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9ef5b1a-1f70-4b34-81fc-0ead9223f2c9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Notebook Parameters\ndatavaultEntityName = dbutils.widgets.get(\"DataVault_Entity_Name\")\nsourceSystem = dbutils.widgets.get(\"Source_System\")\n\n# Set Spark DB\nhubDatabase = 'hub'\nsatDatabase = 'sat'\nlinkDatabase = 'link'\nrefDatabase = 'ref'\n\nhubRootPath = rootPath + hubDatabase\nsatRootPath = rootPath + satDatabase\nlnkRootPath = rootPath + linkDatabase\nrefRootPath = rootPath + refDatabase \n\n# Set Dates\ndate_time = current_timestamp()\ndate = current_date()\n\ncurrent_user = spark.sql(\"select current_user()\").first()[0]\n\n# Set count variables\nnewSatMembersCount = 0\ntotalSatRowCount = 0\nisSateEntity = 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe9491d1-f2a1-4319-9f00-10757ada9ed5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-769586589551494>\u001B[0m in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mrefDatabase\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'ref'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mhubRootPath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrootPath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mhubDatabase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0msatRootPath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrootPath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0msatDatabase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mlnkRootPath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrootPath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlinkDatabase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'rootPath' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'rootPath' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-769586589551494>\u001B[0m in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mrefDatabase\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'ref'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mhubRootPath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrootPath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mhubDatabase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0msatRootPath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrootPath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0msatDatabase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mlnkRootPath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrootPath\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlinkDatabase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'rootPath' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["# Load Data Vault Logic\n\ntry: \n    objectsDF = spark.sql(f\"Select * from v_Load_DV where DataVault_Entity_Name = '{datavaultEntityName}'\")\n    \n    row = objectsDF.first()\n    \n    dataVaultEntityId = row[\"DataVault_Entity_ID\"]\n    datavault_Entity_Name = row[\"DataVault_Entity_Name\"]\n    query_file_path = row[\"Logic_Definition\"]\n    hubBKName = row[\"BK_DV_Object\"]\n    hubName = row[\"Main_Entity_Name\"]\n    sourceSystemName = row[\"Source_System_Name\"]\n    entityLoad = row[\"Entity_Load\"]\n    \n    query_full_file_path - os.path.join(query_folder, query_file_path)\n    \n    with open(query_full_file_path, 'r') as f:\n        query_dv = f.read()\nexcept Exception as Error:\n    \n    status = \"Error\"\n    error_msg = Error\n    str_error_msg = str(error_msg)\n    \n    raise Exception( f\"'{status}' in '{outputFile}' : '{datavaultEntityName}' failed to load metadata with error message '{str_error_msg}' in \")\n        "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e0cd736-04f4-4ac3-aceb-f7d1e2ba911f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# List of Entities and Fields\n\ndvEntityFieldsDF = spark.sql(\n    f\" Select DataVault_Entity_Name, Entity_Type_Name, concat_ws(',', collect_list(Field_Name)) AS Field_Name, 'Metadata' as Source, Main_Entity_Key_Name \\\n                                FROM v_DataVault_Entity_Fields \\\n                                Where DataVault_Entity_Logic = '{datavaultEntityName}' \\\n                                Group by DataVault_Entity_Name, Entity_Type_Name, Main_Entity_Key_Name ORDER BY DataVault_Entity_Name DESC\"). cache()\n\nrowEntityFieldsDF = dvEntityFieldsDF.first()\nhubSIDName = rowEntityFieldsDF[\"Main_Entity_Key_Name\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"497fca27-2336-46e7-a382-82e303645333"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Execute query\ndvEntity = spark.sql(query_dv).filter(f\"{hubBKName} is not NULL)\n \ndvEntityCols = sorted(dvEntity.columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd1f3e7a-9e11-4e23-bdb9-863defb2a6c5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create Reference Table\n\nif entityLoad in ('Reference'):\n    refName = hubName\n    refBKNameAlias = hubBKName\n    refSIDName = hubSIDName\n    \n    dvEntityFieldsRefDF = dvEntityFieldsDF\n    \n    for rowDVE in dvEntityFieldsRefDF.collect():\n        # get fields from query\n        \n        colsEntity = [x.strip() for x in rowDVE[\"Field_Name\"].split(,)]\n        \n        cols = sorted([c for c in colsEntity if c in dvEntityCols])\n        \n        colsCopy = cols.copy() # copy list before adding business key\n        cols.insert(0, refBKNameAlias)\n        \n        # Adding Sequence ID\n        colsRS = colsCopy\n        colsRS.insert(0, refSIDName)\n        \n        ref = spark.read.table(refDatabase + \".\" + refName)\n        \n        refFields = (dvEntity\n                         .withColumn(\"R_Record_Source\", lit(sourceSystemName))\n                         .withColumn(\"R_Load_Date\", lit(date_time))\n                         .withColumn(\"R_ERL_ID\", lit(eTLID))\n                         .withColumn(\"R_Hash_Diff\", udfSha256Python_DF(F.array(colsRS)))\n                         .withColumn(\"R_DQ_Status\", lit(\"Unchecked\"))\n                         .withColumn(\"R_Is_Deleted\", lit(0))).cache()\n        \n        newRefMembers = refFields.join(ref, how = 'left_anti', on[refBKNameAlias]) # identify new member\n        \n        # Check if there are new members\n        newRefMembersCount = newRefMembers.count()\n        newRefMembers = newRefMembers.withColumn(\"R_Is_Deleted\", col(\"R_Is_Deleted\").cast(\"boolean\"))\n        newRefMembers = newRefMembers.drop(\"Last_Modified_Date_Time\", \"Last_Modified_By\", \"S_Is_Deleted\")\n        \n        if (newRefMembersCount > 0):\n            # Add new data\n            (newRefMembers.write.format('delta').mode('append').save(os.path.join(refRootPath, refName.lower())))\n            \n            print(f\" --- {newRefMembersCount} new REF members found. --- \")\n        else:\n            print(\" --- No new REF members have been found. ---\")\n            \n        # Uncache dataframe\n        refFields.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ab98bd0-c274-4afe-bb48-e518f15e930c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creating hub\n\nif entityLoad in ('SatHubLink', 'SatHub'):\n    #Create if table does not exist\n    dfCreateHub(hubName, hubSIDName, hubBKName, hubRootPath)\n    \n    # Read hub table\n    hub = spark.read.table(hubDatabase + \".\" + hubName)\n    \n    #Add Hub standard fields\n    \n    hub_source_df = (dvEntity\n                        .selectExpr(f\"cast({hubBKName} as String) as {hubBKName}\")\n                        .withColumn(\"H_Schema\", lit(schema))\n                        .withColumn(\"H_Record_Source\", lit(sourceSystemName))\n                        .withColumn(\"H_Load_Date\", lit(date_time))\n                        .withColumn(hubSIDName, udfSha256Python_DF(F.array(hubBKName)))\n                        .withColumn(\"H_ETL_ID\", lit(eTLID)))\n    \n    # prepare variables\n    hub_name - hubName.lower()\n    hub_path = os.path.join(hubRootPath, hub_name)\n    hub_table = DeltaTable.forName(spark, f\"{hubDatabase}.{hubName}\")\n    \n    if not hub_table.toDF().take(1) or write_mode == 'overwrite':\n        \n        write_mode == 'overwrite'\n        start_time = datetime.now()\n        print(f\"Performing '{write_mode}' for data in '{hubDatabase}.{hubName}' at {start_time}.\")\n        \n        # load new hub ros\n        (hub_source_df.write.format('delta').mode(write_mode).save(hub_path))\n        \n        end_time = datetime.now()\n        print(f\"Completed '{write_mode}' for data in '{hubDatabase}.{hubName}' at {end_time}.\")\n        \n    else:\n        # Identify new member\n        \n        newHubMembers = hub_source_df.join(\n            hub,\n            how = 'left_anti'\n            on = [hubBkName])\n        \n    # check if data exists\n    if newHubMembers.take(1):\n        print(f\"Beginning {write_mode} of new HUB memers to '{hubDatabase}.{hubName}'.\")\n        \n        (newHubMembers.write.format('delta').mode(write_mode).save(hub_path))\n        \n        print(f\"Completed {write_mode} of new HUB members to '{hubDatabase}.{hubName}'.\")\n        \n    else:\n        print(f\"No new HUB members identified for '{hubDatabase}.{hubName}'.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78d54e5a-d770-4832-bcb4-42a089ca9c93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Create Satellite"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d322baa9-fe1b-4601-9c4a-38d8b1504d72"}}},{"cell_type":"code","source":["\n\nif entityLoad in ('SatHubLink', 'SatHub', 'SatLink', 'Sat'):\n    dvEntityFieldsSateDF = dbEntityFieldsDF.filer(\n        (col(\"Entity_Type_Name\") == \"Satellite\") | (col(\"Entity_Type_Name\") == \"LinkSat\"))\n    \n    for rowDVE in dvEntityFieldsSatDF.collect():\n        \n        # getting fields from query\n        colsEntity = [x.strip() for x in rowDVE[\"Field_Name\"].split(',')]\n        entityName = rowDVE[\"DataVault_Entity_Name\"]\n        entityType = rowDVE[\"Entity_Type_Name\"]\n        source = rowDVE[\"Source\"]\n        \n        # get columns for Data Vault entity\n        cols = sorted([c for c in colsEntity if c in dvEntityCols])\n        colsCopy = cols.copy() #copy list before adding the business key\n        cols.insert(0,hubBKName)\n        if not entityType == 'LinkSat':\n            cols.insert(1, hubsSIDName) # Insert SID\n            \n        # Add SID and Record Source in the list to be used in the hash diff -----------------\n        ### Note: the sequence of the fields in colsRS are very important to calculate the hash key\n        \n        colsRS = colsCopy\n        colsRS.insert(0, hubSIDName) # Insert SID\n        \n        # ----------------Create a Dataframe with extra fields ----------------------\n        if entityType in (\"Satellite\", \"LinkSat\"):\n            sat_source_df = (dvEntity.select(*cols)\n                            .withColumn(\"S_Schema\", lit(schema))\n                            .withColumn(\"S_Record_Source\", lit(sourceSystemName))\n                             .withColumn(\"S_Is_Deleted\", col(\"S_Is_Deleted\").cast(\"string\"))\n                             .withColumn(\"S_Hash_Diff\", udfSha256Python_DF(F.array(colsRS))) # Create the hash key. Check if data is different for not key columns\n                             .withColumn(\"S_Load_Date\", lit(date_time)) # Add Load date\n                             .withColumn(\"S_ETL_ID\", lit(eTLID)) # Add ETL ID\n                             .withColumn(\"S_DQ_Status\", lit(\"Unchecked\")) # Add unchecked dq status for sat\n                             .withColumn(\"S_Load_Date_End\", lit(None)) # End date as NULL\n                            )\n            sat_source_df = sat_source_df.withColumn(hubBKName, sat_source_df[hubBKName].cast(\"string\")).dropDuplicates() # Casting BK_ID as String\n            \n            # ------- LOAD SATELLITE-------------------------\n            \n            # prepare variables\n            entity_name = entityName.lower()\n            entity_type = entityType.lower()\n            sat_path = f\"{satRootPath}/{entity_name}\"\n            sat_table = DeltaTable.forName(spark, f\"{satDatabase}.{entityName}\") # Current Delta table\n            sat_latest_view = spark.sql(f\"SELECT * FROM vw_{entityName}_latest\")\n            \n            if not sat_table.toDF().take(1) or write_mode == \"overwrite\" :\n                \n                # match target schema\n                \n                sat_source_df = sat_source_df.withColumn(\"S_Is_Deleted\", col(\"S_Is_Deleted\"):cast(\"boolean\"))\n                \n                # overwrite and log start time\n                \n                write_mode = \"overwrite\"\n                start_time = datetime.now()\n                print(f\"Performing '{write_mode}' for data in '{satDatabase}.{entityName}' at {start_time}.\")\n                \n                # load new satellite rows\n                (sat_source_df.write\n                .format(\"delta\")\n                .mode(write_mode)\n                .save(sat_path)\n                )\n                \n                end_time = datetime.now()\n                \n                print(f\"Completed '{write_mode}' for data in '{satDatabase}.{entityName}' at {end_time}. \")\n            else:\n                \n                # get max date pior to loading\n                \n                pre_max_load_date_time = spark.sql(f\"SELECT MAX(S_Load_Date) AS pre_max_load_date_time FROM {satDatabase}.{entityName}\").first()[\"pre_max_load_date_time\"]\n                start_time = datetime.now()\n                print(f\"Attempting '{write_mode}' for data in '{satDatabase}.{entityName}' at {start_time}.\")\n                \n                # critical for only inseting where vault rows don't exist - consists of new and updated sat members.\n                \n                new_sat_members = (sat_source_df\n                                  .join(sat_latest_view,\n                                       how = \"left_anti\",\n                                       on = [f\"{hubSIDName}\", \"S_Hash_Diff\"]))\n                .withColumn(\"S_Is_Deleted\", col(\"S_Is_Deleted\").cast(\"boolean\"))\n                \n                (new_sat_members.write\n                .format(\"delta\")\n                .mode(write_mode)\n                .save(sat_path)\n                )\n                \n                end_time = datetime.now()\n                print(f\"Completed '{write_mode}' for data in '{satDatabase}.{entityName}' at {end_time}.\")\n                \n                # get metrics for logging and change detection\n                \n                sat_metric_df = spark.sql(f\"SELECT COUNT(*) AS total_row_count, MAX(S_Load_Data) AS post_max_load_date_time FROM {satDatabase}.{entityName}\").first()\n                for sat_metric in sat_metric_df:\n                    totalSatRowCount = sat_metric_df[\"total_row_count\"]\n                    post_max_load_date_time = sat_metric_df[\"post_max_load_date_time\"]\n                    \n                isSatEntity = 1\n                \n                # re-create latest view with streamed delta data\n                \n                start_time = datetime.now()\n                print(f\"Checking need for delta stream at {start_time}.\")\n                dq_run = spark.sql(f\"SELECT 1 FROM {satDatabase}.{entityName} WHERE S_DQ_Status IN ('Passed', 'Failed')\").take(1)\n                \n                # determine changed data\n                \n                if post_max_load_date_time > pre_max_load_date_time:\n                    print(f\"Checking need for delta stream at {start_time}.\")\n                    \n                    # Get delta count of rows\n                    \n                    newSatMembersCount = spark.sql(f\"SELECT COUNT(*) AS etl_row_count FROM {satDatabase}.{entityName} WHERE S_ETL_Id = {eTLID}\").first()[\"etl_row_count\"]\n                    \n                    \n                    # for DQ purposes\n                    if dq_run:\n                        \n                        # prepare stream\n                        \n                        entity_name = entityName.lower()\n                        entity_type = entityType.lower()\n                        sat_stream_loc = f\"{satRootPath}/{entity_name}/stream\"\n                        sat_stream_table = f\"stream_{entity_name}\"\n                        sat_checkpoint_loc = f\"{sat_stream_loc}/_checkpoint\"\n                        sat_unchecked_view = f\"vw_{entity_name}_latest_unchecked\"\n                        OUTPUT_MODE = \"append\"\n                        \n                        start_time = datetime.now()\n                        print(f\"Begging data '{OUTPUT_MODE}' to '{sat_stream_loc}' at {start_time}.\")\n                        \n                        # read link table to stream out appended data after \"pre_max_load_date_time\"\n                        \n                        sat_read_changes_df = (spark.readStream\n                                              .format(\"delta\")\n                                              .option(\"startingTimestamp\", pre_max_load_date_time)\n                                              .table(f\"{satDatabase}.{entityName}\")\n                                              )\n                        \n                        # write deltas to file\n                        sat_write_changes_df = (sat_read_changes_df.writeStream\n                                               .format(\"delta\")\n                                               .option(\"checkpointLocation\", sat_checkpoint_loc)\n                                               .outputMode(OUTPUT_MODE)\n                                               .trigger(once=True)\n                                               .queryName(f\"qry_delta_{entity_type}_{entity_name}\")\n                                               )\n                        end_time = datetime.now()\n                        print(f\"Completed data '{OUTPUT_MODE}' to '{sat_stream_loc}' at {end_time}.\")\n                    else:\n                        end_time = datetime.now()\n                        print(f\"Changes detected but Delta street not required for data in '{satDatabase}.{entityNAme}' at {end_time}.\")\n                    else:\n                        # no changes detected\n                        end_time = datetime.now()\n                        print(f\"No Changes detected and Delta stream not required for data in '{satDatabase}.{entityName}' at {end_time}.\")\n            \n            \n                                      "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e73d790-7596-436d-96ba-e1bf11bd6f79"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Links"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"819c6dea-9860-4e96-b6cc-c3fcbeb5d95b"}}},{"cell_type":"code","source":["if entityLoad in ('SatHubLink','Link','SatLink'):\n    dvEntityFieldsLinkDF = dvEntityFieldsDf.filter(col(\"Entity_Type_Name\")==\"Link\")\n    \n    for rowDVE in dvEntityFieldsLinkDF.collect():\n        #---------Get the columns from Query i.e queryDV ---------------\n        link_cols = [for x in rowDVE[\"Field_Name\"].split(',')]\n        entityName = rowDVE[\"DataVault_Entity_Name\"]\n        entityType = rowDVE[\"Entity_Type_Name\"]\n        linkSID = rowDVE[\"Main_Entity_Key_Name\"]\n        \n        \n        cols = sorted([c for c in link_cols if c in dvEntityCols]) #check if all columns in the metadata (in case of main sat and links) exists in the Dataframe\n        cols.insert(0, linkSID)\n        \n        \n        #--------------Create a Dataframe with extra fields ---------------\n        link_source_df = (dvEntity.select(*cols)\n                          .withColumn(\"L_Schema\", lit(schema)) #add schema\n                          .withColumn(\"L_Record_Source\", lit(sourceSystemName)) # Add Record Source - Source System Name\n                          .withColumn(\"L_Load_Date\", lit(date_time))\n                          .withColumn(\"L_ETL_ID\", lit(eTLID))\n                         )\n        \n        if entityType == \"link\":\n            link_source_df = link_source_df.dropDuplicates()\n            link_table = DeltaTable.forName(spark, f\"{linkDatabase}.{entityName}\")\n            \n            # prepare variables\n            \n            entity_name = entityName.lower()\n            entity_type = entityType.lower()\n            link_path = f\"{lnkRootPath}/{entity_name}\"\n            \n            if not link_table.toDF().take(1) or write_mode == \"overwrite\":\n                \n                write_mode = \"overwrite\"\n                start_time = datetime.now()\n                print(f\"Performing '{write_mode}' for data in '{linkDatabase}.{entityName}' at {starttime}.\" )\n                \n                (link_source_df\n                 .write\n                .format(\"delta\")\n                .mode(write_mode)\n                .save(link_path)\n                )\n                \n                end_time = datetime.now()\n                print(f\"Completed '{write_mode}' for data in '{linkDatabase}.{entityName}' at {end_time}.\")\n            else:\n                start_time = datetime.now()\n                print(f\"Performing '{write_mode}' for data in '{linkDatabase}.{entityName}' at {start_time}.\")\n                \n                # Append link data\n                (link_table.alias(\"linkCurrent\")\n                .merge(link_source_df.alias(\"linkUpdates\"),\n                      f\"\"\"linkCurrent.{hubSIDName} = linkUpdates.{hubSIDName}\"\"\")\n                .whenNotMatchedInsertAll()\n                .execute()\n                )\n                \n                end_time = datetime.now()\n                print(f\"Completed '{write_mode}' for data in '{linkDatabase}.{entity}' at {end_time}.\")\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b851d628-dde6-414f-8522-5ec54f081d7a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Uncache dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7da14d12-dc33-42ad-8498-042dbb1c6f17"}}},{"cell_type":"code","source":["if entityLoad in ('SatHubLink','SatHub','Link','SatLink','Sat'):\n    objectDF.unpersist()\n    dvEntityFieldsDF.unpersist()\n    dvEntity.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9afb0e5-0845-4809-9f8c-6c5dfd1e9341"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Exit notebook with row counts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e69ec4dc-9a9d-4fb1-9eb0-8a77dae19a96"}}},{"cell_type":"code","source":["# Create Json string to be sent to calling ADF\n# ADF activity can access items as like activity('ActivityName').output.runOutput.processRowCount\n\noutput = json.dumps(\n{\n    \"processedRowCount\": newSatMembersCount,\n    \"totalRowCount\": totalSatRowCount,\n    \"isSatEntity\": str(isSatEntity)\n    \n}\n)\n\ndbutils.notebook.exit(output)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a48968d1-ed8e-4881-ae67-539f7886076c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data Vault","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{"query_folder":{"nuid":"5635beae-a4cc-4d43-bda7-e6c3c400c7bb","currentValue":"/dbfs/mnt/....","widgetInfo":{"widgetType":"text","name":"query_folder","defaultValue":"/dbfs/mnt/....","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Is_Overwrite_Schema":{"nuid":"ad819d73-36ca-429d-852c-7c704084d6f9","currentValue":"False","widgetInfo":{"widgetType":"text","name":"Is_Overwrite_Schema","defaultValue":"False","label":null,"options":{"widgetType":"text","validationRegex":null}}},"ETL_ID":{"nuid":"eb0ae30f-772b-4c38-92a3-405471b0df14","currentValue":"","widgetInfo":{"widgetType":"text","name":"ETL_ID","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Data_Factory_Name":{"nuid":"a22b1044-8a35-4383-95f8-aa2d3df2c58b","currentValue":"","widgetInfo":{"widgetType":"text","name":"Data_Factory_Name","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Pipeline_Run_Id":{"nuid":"63234e23-2ef4-4828-ab58-0dc35460e18c","currentValue":"","widgetInfo":{"widgetType":"text","name":"Pipeline_Run_Id","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Is_Merge_Schema":{"nuid":"3aab74ff-27c0-48fa-8dfb-88fed4d5c046","currentValue":"True","widgetInfo":{"widgetType":"text","name":"Is_Merge_Schema","defaultValue":"True","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Root_Path":{"nuid":"eabae51e-c660-40ef-8dfd-a834a59f58d9","currentValue":"s3","widgetInfo":{"widgetType":"text","name":"Root_Path","defaultValue":"s3","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Pipeline_Name":{"nuid":"525e8f8b-8689-41b7-8ab3-3c5a1bec5243","currentValue":"","widgetInfo":{"widgetType":"text","name":"Pipeline_Name","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Write_Mode":{"nuid":"0bd536ea-24fd-41a8-8f82-bca44c091b5e","currentValue":"append","widgetInfo":{"widgetType":"text","name":"Write_Mode","defaultValue":"append","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Deletion_Capture_Enabled":{"nuid":"093a6c03-3610-496e-ab54-ca7425e93236","currentValue":"","widgetInfo":{"widgetType":"text","name":"Deletion_Capture_Enabled","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Is_Full_Refresh":{"nuid":"c3703e31-68c4-4a70-9b29-edc455cc13c9","currentValue":"","widgetInfo":{"widgetType":"text","name":"Is_Full_Refresh","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Schema":{"nuid":"628239de-9f4e-48d8-8096-6dc3b5a462bf","currentValue":"","widgetInfo":{"widgetType":"text","name":"Schema","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Source_System":{"nuid":"b3de726e-c3cd-4852-9019-9ab36c851965","currentValue":"","widgetInfo":{"widgetType":"text","name":"Source_System","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}},"DataVault_Entity_Name":{"nuid":"45102f1b-1335-4f47-9521-1750f8adb4e5","currentValue":"","widgetInfo":{"widgetType":"text","name":"DataVault_Entity_Name","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":578448455515501}},"nbformat":4,"nbformat_minor":0}
